{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils.data_prepare import load_class_list\n",
    "class_list = load_class_list()\n",
    "len(class_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import get_dataloader\n",
    "train_loader, val_loader = get_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch:  0\n",
      "img.shape:  torch.Size([32, 3, 224, 224])\n",
      "label.shape:  torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    print(\"batch: \", i_batch)\n",
    "    print(\"img.shape: \", sample_batched[0].shape)\n",
    "    print(\"label.shape: \", sample_batched[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "resnet50 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0570,  0.0672,  0.0206,  ..., -0.0104, -0.0206, -0.0793],\n",
       "        [ 0.0071,  0.0121, -0.0378,  ...,  0.0190,  0.0385, -0.0190],\n",
       "        [-0.0229, -0.0401,  0.0095,  ..., -0.0403,  0.0005, -0.0602],\n",
       "        ...,\n",
       "        [-0.0473,  0.0435,  0.0457,  ..., -0.0287,  0.0418,  0.0169],\n",
       "        [ 0.0253,  0.0187, -0.0010,  ..., -0.0160,  0.0155, -0.0023],\n",
       "        [-0.0503, -0.0725,  0.0680,  ...,  0.0709,  0.0063,  0.0205]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "resnet50.fc = nn.Linear(2048,102)\n",
    "nn.init.xavier_normal_(resnet50.fc.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in resnet50.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in resnet50.fc.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ResNet                                   [1, 102]                  --\n",
       "├─Conv2d: 1-1                            [1, 64, 112, 112]         (9,408)\n",
       "├─BatchNorm2d: 1-2                       [1, 64, 112, 112]         (128)\n",
       "├─ReLU: 1-3                              [1, 64, 112, 112]         --\n",
       "├─MaxPool2d: 1-4                         [1, 64, 56, 56]           --\n",
       "├─Sequential: 1-5                        [1, 256, 56, 56]          --\n",
       "│    └─Bottleneck: 2-1                   [1, 256, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-1                  [1, 64, 56, 56]           (4,096)\n",
       "│    │    └─BatchNorm2d: 3-2             [1, 64, 56, 56]           (128)\n",
       "│    │    └─ReLU: 3-3                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-4                  [1, 64, 56, 56]           (36,864)\n",
       "│    │    └─BatchNorm2d: 3-5             [1, 64, 56, 56]           (128)\n",
       "│    │    └─ReLU: 3-6                    [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-7                  [1, 256, 56, 56]          (16,384)\n",
       "│    │    └─BatchNorm2d: 3-8             [1, 256, 56, 56]          (512)\n",
       "│    │    └─Sequential: 3-9              [1, 256, 56, 56]          (16,896)\n",
       "│    │    └─ReLU: 3-10                   [1, 256, 56, 56]          --\n",
       "│    └─Bottleneck: 2-2                   [1, 256, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-11                 [1, 64, 56, 56]           (16,384)\n",
       "│    │    └─BatchNorm2d: 3-12            [1, 64, 56, 56]           (128)\n",
       "│    │    └─ReLU: 3-13                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-14                 [1, 64, 56, 56]           (36,864)\n",
       "│    │    └─BatchNorm2d: 3-15            [1, 64, 56, 56]           (128)\n",
       "│    │    └─ReLU: 3-16                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-17                 [1, 256, 56, 56]          (16,384)\n",
       "│    │    └─BatchNorm2d: 3-18            [1, 256, 56, 56]          (512)\n",
       "│    │    └─ReLU: 3-19                   [1, 256, 56, 56]          --\n",
       "│    └─Bottleneck: 2-3                   [1, 256, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-20                 [1, 64, 56, 56]           (16,384)\n",
       "│    │    └─BatchNorm2d: 3-21            [1, 64, 56, 56]           (128)\n",
       "│    │    └─ReLU: 3-22                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-23                 [1, 64, 56, 56]           (36,864)\n",
       "│    │    └─BatchNorm2d: 3-24            [1, 64, 56, 56]           (128)\n",
       "│    │    └─ReLU: 3-25                   [1, 64, 56, 56]           --\n",
       "│    │    └─Conv2d: 3-26                 [1, 256, 56, 56]          (16,384)\n",
       "│    │    └─BatchNorm2d: 3-27            [1, 256, 56, 56]          (512)\n",
       "│    │    └─ReLU: 3-28                   [1, 256, 56, 56]          --\n",
       "├─Sequential: 1-6                        [1, 512, 28, 28]          --\n",
       "│    └─Bottleneck: 2-4                   [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-29                 [1, 128, 56, 56]          (32,768)\n",
       "│    │    └─BatchNorm2d: 3-30            [1, 128, 56, 56]          (256)\n",
       "│    │    └─ReLU: 3-31                   [1, 128, 56, 56]          --\n",
       "│    │    └─Conv2d: 3-32                 [1, 128, 28, 28]          (147,456)\n",
       "│    │    └─BatchNorm2d: 3-33            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-34                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-35                 [1, 512, 28, 28]          (65,536)\n",
       "│    │    └─BatchNorm2d: 3-36            [1, 512, 28, 28]          (1,024)\n",
       "│    │    └─Sequential: 3-37             [1, 512, 28, 28]          (132,096)\n",
       "│    │    └─ReLU: 3-38                   [1, 512, 28, 28]          --\n",
       "│    └─Bottleneck: 2-5                   [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-39                 [1, 128, 28, 28]          (65,536)\n",
       "│    │    └─BatchNorm2d: 3-40            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-41                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-42                 [1, 128, 28, 28]          (147,456)\n",
       "│    │    └─BatchNorm2d: 3-43            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-44                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-45                 [1, 512, 28, 28]          (65,536)\n",
       "│    │    └─BatchNorm2d: 3-46            [1, 512, 28, 28]          (1,024)\n",
       "│    │    └─ReLU: 3-47                   [1, 512, 28, 28]          --\n",
       "│    └─Bottleneck: 2-6                   [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-48                 [1, 128, 28, 28]          (65,536)\n",
       "│    │    └─BatchNorm2d: 3-49            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-50                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-51                 [1, 128, 28, 28]          (147,456)\n",
       "│    │    └─BatchNorm2d: 3-52            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-53                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-54                 [1, 512, 28, 28]          (65,536)\n",
       "│    │    └─BatchNorm2d: 3-55            [1, 512, 28, 28]          (1,024)\n",
       "│    │    └─ReLU: 3-56                   [1, 512, 28, 28]          --\n",
       "│    └─Bottleneck: 2-7                   [1, 512, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-57                 [1, 128, 28, 28]          (65,536)\n",
       "│    │    └─BatchNorm2d: 3-58            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-59                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-60                 [1, 128, 28, 28]          (147,456)\n",
       "│    │    └─BatchNorm2d: 3-61            [1, 128, 28, 28]          (256)\n",
       "│    │    └─ReLU: 3-62                   [1, 128, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-63                 [1, 512, 28, 28]          (65,536)\n",
       "│    │    └─BatchNorm2d: 3-64            [1, 512, 28, 28]          (1,024)\n",
       "│    │    └─ReLU: 3-65                   [1, 512, 28, 28]          --\n",
       "├─Sequential: 1-7                        [1, 1024, 14, 14]         --\n",
       "│    └─Bottleneck: 2-8                   [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-66                 [1, 256, 28, 28]          (131,072)\n",
       "│    │    └─BatchNorm2d: 3-67            [1, 256, 28, 28]          (512)\n",
       "│    │    └─ReLU: 3-68                   [1, 256, 28, 28]          --\n",
       "│    │    └─Conv2d: 3-69                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-70            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-71                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-72                 [1, 1024, 14, 14]         (262,144)\n",
       "│    │    └─BatchNorm2d: 3-73            [1, 1024, 14, 14]         (2,048)\n",
       "│    │    └─Sequential: 3-74             [1, 1024, 14, 14]         (526,336)\n",
       "│    │    └─ReLU: 3-75                   [1, 1024, 14, 14]         --\n",
       "│    └─Bottleneck: 2-9                   [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-76                 [1, 256, 14, 14]          (262,144)\n",
       "│    │    └─BatchNorm2d: 3-77            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-78                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-79                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-80            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-81                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-82                 [1, 1024, 14, 14]         (262,144)\n",
       "│    │    └─BatchNorm2d: 3-83            [1, 1024, 14, 14]         (2,048)\n",
       "│    │    └─ReLU: 3-84                   [1, 1024, 14, 14]         --\n",
       "│    └─Bottleneck: 2-10                  [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-85                 [1, 256, 14, 14]          (262,144)\n",
       "│    │    └─BatchNorm2d: 3-86            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-87                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-88                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-89            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-90                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-91                 [1, 1024, 14, 14]         (262,144)\n",
       "│    │    └─BatchNorm2d: 3-92            [1, 1024, 14, 14]         (2,048)\n",
       "│    │    └─ReLU: 3-93                   [1, 1024, 14, 14]         --\n",
       "│    └─Bottleneck: 2-11                  [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-94                 [1, 256, 14, 14]          (262,144)\n",
       "│    │    └─BatchNorm2d: 3-95            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-96                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-97                 [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-98            [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-99                   [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-100                [1, 1024, 14, 14]         (262,144)\n",
       "│    │    └─BatchNorm2d: 3-101           [1, 1024, 14, 14]         (2,048)\n",
       "│    │    └─ReLU: 3-102                  [1, 1024, 14, 14]         --\n",
       "│    └─Bottleneck: 2-12                  [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-103                [1, 256, 14, 14]          (262,144)\n",
       "│    │    └─BatchNorm2d: 3-104           [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-105                  [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-106                [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-107           [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-108                  [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-109                [1, 1024, 14, 14]         (262,144)\n",
       "│    │    └─BatchNorm2d: 3-110           [1, 1024, 14, 14]         (2,048)\n",
       "│    │    └─ReLU: 3-111                  [1, 1024, 14, 14]         --\n",
       "│    └─Bottleneck: 2-13                  [1, 1024, 14, 14]         --\n",
       "│    │    └─Conv2d: 3-112                [1, 256, 14, 14]          (262,144)\n",
       "│    │    └─BatchNorm2d: 3-113           [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-114                  [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-115                [1, 256, 14, 14]          (589,824)\n",
       "│    │    └─BatchNorm2d: 3-116           [1, 256, 14, 14]          (512)\n",
       "│    │    └─ReLU: 3-117                  [1, 256, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-118                [1, 1024, 14, 14]         (262,144)\n",
       "│    │    └─BatchNorm2d: 3-119           [1, 1024, 14, 14]         (2,048)\n",
       "│    │    └─ReLU: 3-120                  [1, 1024, 14, 14]         --\n",
       "├─Sequential: 1-8                        [1, 2048, 7, 7]           --\n",
       "│    └─Bottleneck: 2-14                  [1, 2048, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-121                [1, 512, 14, 14]          (524,288)\n",
       "│    │    └─BatchNorm2d: 3-122           [1, 512, 14, 14]          (1,024)\n",
       "│    │    └─ReLU: 3-123                  [1, 512, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-124                [1, 512, 7, 7]            (2,359,296)\n",
       "│    │    └─BatchNorm2d: 3-125           [1, 512, 7, 7]            (1,024)\n",
       "│    │    └─ReLU: 3-126                  [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-127                [1, 2048, 7, 7]           (1,048,576)\n",
       "│    │    └─BatchNorm2d: 3-128           [1, 2048, 7, 7]           (4,096)\n",
       "│    │    └─Sequential: 3-129            [1, 2048, 7, 7]           (2,101,248)\n",
       "│    │    └─ReLU: 3-130                  [1, 2048, 7, 7]           --\n",
       "│    └─Bottleneck: 2-15                  [1, 2048, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-131                [1, 512, 7, 7]            (1,048,576)\n",
       "│    │    └─BatchNorm2d: 3-132           [1, 512, 7, 7]            (1,024)\n",
       "│    │    └─ReLU: 3-133                  [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-134                [1, 512, 7, 7]            (2,359,296)\n",
       "│    │    └─BatchNorm2d: 3-135           [1, 512, 7, 7]            (1,024)\n",
       "│    │    └─ReLU: 3-136                  [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-137                [1, 2048, 7, 7]           (1,048,576)\n",
       "│    │    └─BatchNorm2d: 3-138           [1, 2048, 7, 7]           (4,096)\n",
       "│    │    └─ReLU: 3-139                  [1, 2048, 7, 7]           --\n",
       "│    └─Bottleneck: 2-16                  [1, 2048, 7, 7]           --\n",
       "│    │    └─Conv2d: 3-140                [1, 512, 7, 7]            (1,048,576)\n",
       "│    │    └─BatchNorm2d: 3-141           [1, 512, 7, 7]            (1,024)\n",
       "│    │    └─ReLU: 3-142                  [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-143                [1, 512, 7, 7]            (2,359,296)\n",
       "│    │    └─BatchNorm2d: 3-144           [1, 512, 7, 7]            (1,024)\n",
       "│    │    └─ReLU: 3-145                  [1, 512, 7, 7]            --\n",
       "│    │    └─Conv2d: 3-146                [1, 2048, 7, 7]           (1,048,576)\n",
       "│    │    └─BatchNorm2d: 3-147           [1, 2048, 7, 7]           (4,096)\n",
       "│    │    └─ReLU: 3-148                  [1, 2048, 7, 7]           --\n",
       "├─AdaptiveAvgPool2d: 1-9                 [1, 2048, 1, 1]           --\n",
       "├─Linear: 1-10                           [1, 102]                  208,998\n",
       "==========================================================================================\n",
       "Total params: 23,717,030\n",
       "Trainable params: 208,998\n",
       "Non-trainable params: 23,508,032\n",
       "Total mult-adds (G): 4.09\n",
       "==========================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 177.82\n",
       "Params size (MB): 94.87\n",
       "Estimated Total Size (MB): 273.29\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(resnet50, input_size=(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def score(model, dataloader, device):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            out = model(x)\n",
    "            y_pred.extend(torch.argmax(out, dim=-1).tolist())\n",
    "            y_true.extend(y.tolist())\n",
    "    accuracy = sum([1 if y_true[i] == y_pred[i] else 0 for i in range(len(y_true))]) / len(y_true)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "resnet50.to(device)\n",
    "\n",
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet50.fc.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [1/225], Loss: 4.7531\n",
      "Epoch [1/10], Step [21/225], Loss: 3.8426\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\dongl\\Desktop\\aistudio\\demo1\\2_resnet50.ipynb Cell 15\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dongl/Desktop/aistudio/demo1/2_resnet50.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dongl/Desktop/aistudio/demo1/2_resnet50.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/dongl/Desktop/aistudio/demo1/2_resnet50.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m outputs \u001b[39m=\u001b[39m resnet50(data)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/dongl/Desktop/aistudio/demo1/2_resnet50.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/dongl/Desktop/aistudio/demo1/2_resnet50.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\dongl\\.conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dongl\\.conda\\envs\\d2l\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[1;32mc:\\Users\\dongl\\.conda\\envs\\d2l\\lib\\site-packages\\torchvision\\models\\resnet.py:273\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    270\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m    271\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmaxpool(x)\n\u001b[1;32m--> 273\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(x)\n\u001b[0;32m    274\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[0;32m    275\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n",
      "File \u001b[1;32mc:\\Users\\dongl\\.conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dongl\\.conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dongl\\.conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dongl\\.conda\\envs\\d2l\\lib\\site-packages\\torchvision\\models\\resnet.py:154\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    151\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(out)\n\u001b[0;32m    152\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[1;32m--> 154\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv3(out)\n\u001b[0;32m    155\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn3(out)\n\u001b[0;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dongl\\.conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\dongl\\.conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\dongl\\.conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    454\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = resnet50(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        if i % 20 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "            \n",
    "    # Evaluate\n",
    "    train_acc = score(resnet50, train_loader, device)\n",
    "    val_acc = score(resnet50, val_loader, device)\n",
    "    print(\"Epoch [{}/{}], Train Acc: {:.4f}, Val Acc: {:.4f}\".format(epoch+1, num_epochs, train_acc, val_acc))\n",
    "\n",
    "    # Save the model checkpoint\n",
    "    if (epoch+1) % 2 == 0:\n",
    "        torch.save(resnet50.state_dict(), 'resnet50_epoch{}.pth'.format(epoch+1))\n",
    "\n",
    "    # Decay learning rate\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
